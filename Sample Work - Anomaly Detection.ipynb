{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                   ANOMALY DETECTION USING K-MEANS CLUSTERING\n",
    "\n",
    "CHINMAY BAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "spark = SparkSession.builder.appName('K_Means_Clustering').getOrCreate()\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "pd_df = pd.read_csv(r'C:\\Users\\chinm\\Downloads\\Subscribers.csv')\n",
    "main_df = spark.read.csv(r'C:\\Users\\chinm\\Downloads\\Subscribers.csv',inferSchema=True,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESCRIPTION OF THE PROBLEM**\n",
    "\n",
    "We have time-series data of the count of subscribers in a telecom organisation. Our objective is to build an Anomaly Detecting model which could help define a threshold or a baseline for customer churn operations and specifically assist in distinguishing seasonal data from anomalous data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|ACTIVATION_DATE|ALL_SUBSCRIBERS|\n",
      "+---------------+---------------+\n",
      "|01-05-2020     |7131           |\n",
      "|02-05-2020     |6668           |\n",
      "|03-05-2020     |4155           |\n",
      "|04-05-2020     |5979           |\n",
      "|05-05-2020     |5835           |\n",
      "|06-05-2020     |5849           |\n",
      "|07-05-2020     |5775           |\n",
      "|08-05-2020     |5954           |\n",
      "|09-05-2020     |5752           |\n",
      "|10-05-2020     |3803           |\n",
      "+---------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPROACH\n",
    "\n",
    "We first train a K-Means cusltering model using Spark ML. It includes evaluating optimal value for k, training the model with that optimal value and returning cluster centroids from the trained model\n",
    "\n",
    "![alt text](an1.PNG \"title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(transformed_df):\n",
    "    \n",
    "    errors = []\n",
    "    sil = []\n",
    "    val_list = []\n",
    "    ctr = []\n",
    "\n",
    "    for k in range(2,10): \n",
    "        kmeans = KMeans(featuresCol='features',k=k).setSeed(2)\n",
    "        model = kmeans.fit(transformed_df)\n",
    "        intra_distance = model.computeCost(transformed_df)\n",
    "        errors.append(intra_distance)\n",
    "        \n",
    "        predictions = model.transform(transformed_df)\n",
    "        evaluatorObj = ClusteringEvaluator()\n",
    "        sil.append(evaluatorObj.evaluate(predictions))\n",
    "        \n",
    "     \n",
    "        index = errors.index(min(errors))\n",
    "        \n",
    "        if index == errors.index(errors[-1]) or index == errors.index(errors[0]):\n",
    "            k_value = index\n",
    "            \n",
    "        else:\n",
    "            for value in [index,index - 1,index + 1]:\n",
    "                val_list.append(sil[value])\n",
    "            k_value = val_list.index(max(val_list))\n",
    "   \n",
    "        \n",
    "    kmeans = KMeans(featuresCol='features',k=k_value)\n",
    "    model = kmeans.fit(transformed_df)\n",
    "    predictions = model.transform(transformed_df) \n",
    "    centers = model.clusterCenters()\n",
    "    \n",
    "    for center in centers:\n",
    "        ctr.append(center)\n",
    "        \n",
    "    return ctr \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALGORITHM FOR ANOMALY DETECTION \n",
    "\n",
    "![alt text](anomaly3.PNG \"Title\")\n",
    "\n",
    "Threshold values could impact the precision and recall of the model. Smaller threhold values could result in a more sensitive detection mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly(ctr,sub_col,testing_df):\n",
    "\n",
    "    for x in range(0,len(ctr),1):\n",
    "\n",
    "        df1 = pd.DataFrame()\n",
    "        dp =  list()\n",
    "\n",
    "        for k in np.sqrt((ctr[x]-sub_col)**2): \n",
    "            if k > np.percentile(np.sqrt((ctr[x]-sub_col)**2),95):\n",
    "                dp.append(np.where(np.sqrt((ctr[x]-sub_col)**2) == k))\n",
    "\n",
    "        for j in range(0,len(dp),1):\n",
    "                df1 = df1.append(testing_df.loc[dp[j][0][0],:])\n",
    "                \n",
    "        return df1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](an2.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note: the above graphics were generated in the analysis phase, and are not generated by any of the given code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(): \n",
    "    \n",
    "    main_df = spark.read.csv(r'C:\\Users\\chinm\\Downloads\\Subscribers.csv',inferSchema=True,header=True)\n",
    "    training_df,testing_df = main_df.randomSplit([0.85,0.15])\n",
    "    testing_df = testing_df.toPandas()\n",
    "\n",
    "    input_cols = ['ALL_SUBSCRIBERS']\n",
    "    vec_assembler = VectorAssembler(inputCols = input_cols, outputCol=\"features\")\n",
    "    transformed_df = vec_assembler.transform(training_df)\n",
    "\n",
    "\n",
    "    ctr = k_means(transformed_df)\n",
    "\n",
    "    sub_col = testing_df['ALL_SUBSCRIBERS']\n",
    "    sub_col = np.array(sub_col)\n",
    "    \n",
    "\n",
    "    anomalies = anomaly(ctr,sub_col,testing_df)\n",
    "    \n",
    "    return anomalies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTIVATION_DATE</th>\n",
       "      <th>ALL_SUBSCRIBERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>08-11-2020</td>\n",
       "      <td>3464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>15-11-2020</td>\n",
       "      <td>3429.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACTIVATION_DATE  ALL_SUBSCRIBERS\n",
       "8       08-11-2020           3464.0\n",
       "18      15-11-2020           3429.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS OF THE RESULT\n",
    "\n",
    "The predict function returns data points classified as anomalous by the anomaly function defined above. Accuracy of the prediction is contextual. If in future, the model classifies data points with similar dates, or rather time of the year (for a yearly time-series), then the decision making process could pin point towards such dates, further identifying any trend or pattern in them. As against that, if certain data points have very unqiue time occurences, then such instances could be invesitaged for other impacting factors. \n",
    "\n",
    "### REFERENCES\n",
    "\n",
    "- https://www.janospoor.com/posts/2019-07-20-detecting-process-anomalies/\n",
    "- https://www.youtube.com/watch?v=69QYeZC_dFU&list=PLeEuH8so9u0JYokE0gY-tpwI6Cper9k6C&index=6&t=2296s\n",
    "- https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
